---
title: "Peer Assessment II"
output:
  html_document: 
    pandoc_args: [
      "--number-sections",
    ]
---

# Background

As a statistical consultant working for a real estate investment firm, your task is to develop a model to predict the selling price of a given home in Ames, Iowa. Your employer hopes to use this information to help assess whether the asking price of a house is higher or lower than the true value of the house. If the home is undervalued, it may be a good investment for the firm.

# Training Data and relevant packages

In order to better assess the quality of the model you will produce, the data have been randomly divided into three separate pieces: a training data set, a testing data set, and a validation data set. For now we will load the training data set, the others will be loaded and used later.

```{r load, message = FALSE}
load("ames_train.Rdata")
```

Use the code block below to load any necessary packages

```{r packages, message = FALSE}
library(statsr)
library(dplyr)
library(BAS)
library(tidyverse)
library(MASS)
```

## Part 1 - Exploratory Data Analysis (EDA)

When you first get your data, it's very tempting to immediately begin fitting models and assessing how they perform.  However, before you begin modeling, it's absolutely essential to explore the structure of the data and the relationships between the variables in the data set.

Do a detailed EDA of the ames_train data set, to learn about the structure of the data and the relationships between the variables in the data set (refer to Introduction to Probability and Data, Week 2, for a reminder about EDA if needed). Your EDA should involve creating and reviewing many plots/graphs and considering the patterns and relationships you see. 

After you have explored completely, submit the three graphs/plots that you found most informative during your EDA process, and briefly explain what you learned from each (why you found each informative).

* * *

```{r}
summary(ames_train)
```

Let's first clean the data.

The categorical variables which are encoded as type int have to be converted to factors first.

* MS.SubClass
* Overall.Qual
* Overall.Cond

```{r}
str(ames_train$MS.SubClass)
```

```{r}
str(ames_train$Overall.Cond)
```

```{r}
str(ames_train$Overall.Qual)
```

Convert the above three variables to factors:

```{r}
ames_train <- ames_train %>% mutate(MS.SubClass = as.factor(MS.SubClass), Overall.Qual = as.factor(Overall.Qual), Overall.Cond = as.factor(Overall.Cond))
```

Transformation of NA's to a new category will avoid bias in the data and the modelling by removing data from the dataset.

Lot.Frontage variable is a continuous variable which has 167 NA's (missing data). Hence, we shall not transform Lot.Frontage variable. 

But other variables such as , Alley, Bsmt.Qual, Bsmt.Cond, Bsmt.Exposure, BsmtFin.Type.1, BsmtFin.Type.2, Fireplace.Qu, Garage.Type, Garage.Finish,Garage.Qual,Garage.Cond,Pool.QC, Fence, Misc.Feature are categorical variables which has NA's that should be converted to a new category.

```{r}
ames_train <- ames_train %>% mutate(
  Alley = if_else(is.na(Alley), 'No Alley', as.character(Alley)),
  Bsmt.Qual = if_else(is.na(Bsmt.Qual), 'No Basement', as.character(Bsmt.Qual)),
  Bsmt.Cond = if_else(is.na(Bsmt.Cond), 'No Basement', as.character(Bsmt.Cond)),
  Bsmt.Exposure = if_else(is.na(Bsmt.Exposure), 'No Basement', as.character(Bsmt.Cond)),
  BsmtFin.Type.1 = if_else(is.na(BsmtFin.Type.1), 'No Basement', as.character(BsmtFin.Type.1)),
  BsmtFin.Type.2 = if_else(is.na(BsmtFin.Type.2), 'No Basement', as.character(BsmtFin.Type.2)),
  Fireplace.Qu = if_else(is.na(Fireplace.Qu), 'No Fireplace', as.character(Fireplace.Qu)),
  Garage.Type = if_else(is.na(Garage.Type), 'No Garage', as.character(Garage.Type)),
  Garage.Finish = if_else(is.na(Garage.Finish), 'No Garage', as.character(Garage.Finish)),
  Garage.Qual = if_else(is.na(Garage.Qual), 'No Garage', as.character(Garage.Qual)),
  Garage.Cond = if_else(is.na(Garage.Cond), 'No Garage', as.character(Garage.Cond)),
  Pool.QC = if_else(is.na(Pool.QC), 'No Pool', as.character(Pool.QC)),
  Fence = if_else(is.na(Fence), 'No Fence', as.character(Fence)),
  Misc.Feature = if_else(is.na(Misc.Feature), 'No MiscFeature', as.character(Misc.Feature))

)
```

At last, let's filter the data set to contain only "Normal" category in Sale.Condition variable.

```{r}
ames_train <- ames_train %>% filter(Sale.Condition == 'Normal')
```

Now, that leaves us 834 observations from 1000 observations.
```{r}
nrow(ames_train)
```

Let's do some Exploratory Data Analysis on the cleaned training data set ames_train.

Graph1: Relationship between price Vs Year.Built

```{r creategraphs}
ames_train %>% ggplot(aes(x = Year.Built, y = price)) + geom_point()
```

The relationship appears to be positive and it's clear from the above plot that new houses have higher price when compared to the old houses. I'm interested in this graph, as it shows clear view about the price in accordance with the year the house was built.

Graph2: Relationship between price Vs. Overall Quality

```{r}
ames_train %>% group_by(Overall.Qual) %>% summarize(median = median(log(price))) %>% ggplot(aes(x = Overall.Qual, y = median)) + geom_point() + geom_smooth(se = FALSE,method = "lm") + labs(title = "Overall Quality Vs Median Price of homes in AMES", x = "Overall Quality", y = "Median Price of Home")
```

From the above plot, it's really clear that the price of a home increases as the overall quality increases.

Graph3: Relationship between Overall Quality , Median Price, with reference to Land Slope Category

```{r}
ames_train %>% group_by(Overall.Qual, Land.Slope) %>% summarize(meanPrice = mean(price)) %>% ggplot(aes(x = Overall.Qual, y = meanPrice, fill = Land.Slope)) + geom_bar(stat = "identity") + labs(title = "Overall Quality Vs Median Price with Reference to Land Slope Category", x = "Overall Quality with Reference to Land Slope", y = "Median Price", fill = "Land Slope")
```

With reference to the previous plot, I just wanted to check how price is distributed with respect to overall quality and land slope category.

* * *

## Part 2 - Development and assessment of an initial model, following a semi-guided process of analysis

### Section 2.1 An Initial Model
In building a model, it is often useful to start by creating a simple, intuitive initial model based on the results of the exploratory data analysis. (Note: The goal at this stage is **not** to identify the "best" possible model but rather to choose a reasonable and understandable starting point. Later you will expand and revise this model to create your final model.

Based on your EDA, select *at most* 10 predictor variables from “ames_train” and create a linear model for `price` (or a transformed version of price) using those variables. Provide the *R code* and the *summary output table* for your model, a *brief justification* for the variables you have chosen, and a *brief discussion* of the model results in context (focused on the variables that appear to be important predictors and how they relate to sales price).

* * *

```{r fit_model}
initialModel <- lm(log(price) ~ Overall.Qual + log(Garage.Area + 1) +   
                  log(Total.Bsmt.SF + 1) + Garage.Cars + log(area) + 
                  Central.Air + 
                  Bedroom.AbvGr + Year.Built +
                  log(Lot.Area) +  Overall.Cond,
                 data = ames_train)
```

`Summary Table for the Initial Linear Regression Model:`

```{r}
summary(initialModel)
```


* * *

### Section 2.2 Model Selection

Now either using `BAS` another stepwise selection procedure choose the "best" model you can, using your initial model as your starting point. Try at least two different model selection methods and compare their results. Do they both arrive at the same model or do they disagree? What do you think this means?

* * *

`First Model Selection: Using AIC`

```{r}
initialModel.AIC <- stepAIC(initialModel, k = 2, direction = "backward", trace = FALSE)
```

```{r}
summary(initialModel.AIC)
```

`Second Model Selection : using BIC`

```{r}
initialModel.BIC <- stepAIC(initialModel, k = log(nrow(ames_train)), direction = "backward", trace = FALSE)
```

```{r}
summary(initialModel.BIC)
```

`Both the model selection do arrive at the same models.`

`In the model selection using AIC and BIC, the predictors remained same yielding both an adjusted r-squared value of 0.9086`

`Thus both the models explains 90.86% percent of variability in the target variable.`

Let's fit the bayesian model also:

```{r}
basModel <- bas.lm(log(price) ~ Overall.Qual +   
                  log(Total.Bsmt.SF + 1) + Garage.Cars + log(area) + 
                  Central.Air + 
                  Bedroom.AbvGr + Year.Built +
                  log(Lot.Area) +  Overall.Cond,
                 data = ames_train, prior = "AIC", modelprior=uniform())
```


```{r}
summary(basModel)
```

Looking at the coefficients of bas model:

```{r}
coefBasModel <- coef(basModel)
coefBasModel
```


* * *

### Section 2.3 Initial Model Residuals
One way to assess the performance of a model is to examine the model's residuals. In the space below, create a residual plot for your preferred model from above and use it to assess whether your model appears to fit the data well. Comment on any interesting structure in the residual plot (trend, outliers, etc.) and briefly discuss potential implications it may have for your model and inference / prediction you might produce.

* * *

```{r model_resid}
predInitialModel.BIC <- exp(predict(initialModel.BIC, ames_train))
```

```{r}
residualInitialModel.BIC <- ames_train$price - predInitialModel.BIC
```

```{r}
plot(residualInitialModel.BIC)
```

plot for predicted Values Vs True Values in the Training Data Set:

```{r}
plot(log(ames_train$price) ~ initialModel.BIC$fitted.values)
```

```{r}
plot(log(ames_train$price) ~ initialModel.AIC$fitted.values)
```

```{r}
plot(initialModel.BIC)
```


* * *

### Section 2.4 Initial Model RMSE

You can calculate it directly based on the model output. Be specific about the units of your RMSE (depending on whether you transformed your response variable). The value you report will be more meaningful if it is in the original units (dollars).

* * *

```{r model_rmse}
predInitialModel.BIC <-exp(predict(initialModel.BIC,ames_train)) 
initialModel.BIC.resid <- ames_train$price - predInitialModel.BIC
intialModel.BIC.rmse <- sqrt(mean(initialModel.BIC.resid^2))
paste("The in-sample RMSE (Root Mean Squared Error) is $", format(round(intialModel.BIC.rmse)), "dollars")
```

* * *

### Section 2.5 Overfitting 

The process of building a model generally involves starting with an initial model (as you have done above), identifying its shortcomings, and adapting the model accordingly. This process may be repeated several times until the model fits the data reasonably well. However, the model may do well on training data but perform poorly out-of-sample (meaning, on a dataset other than the original training data) because the model is overly-tuned to specifically fit the training data. This is called “overfitting.” To determine whether overfitting is occurring on a model, compare the performance of a model on both in-sample and out-of-sample data sets. To look at performance of your initial model on out-of-sample data, you will use the data set `ames_test`.

```{r loadtest, message = FALSE}
load("ames_test.Rdata")
```

Use your model from above to generate predictions for the housing prices in the test data set.  Are the predictions significantly more accurate (compared to the actual sales prices) for the training data than the test data?  Why or why not? Briefly explain how you determined that (what steps or processes did you use)?

* * *

Let's first clean the test data:

```{r}
ames_test <- ames_test %>% mutate(MS.SubClass = as.factor(MS.SubClass), Overall.Qual = as.factor(Overall.Qual), Overall.Cond = as.factor(Overall.Cond))
```

```{r}
ames_test <- ames_test %>% mutate(
  Alley = if_else(is.na(Alley), 'No Alley', as.character(Alley)),
  Bsmt.Qual = if_else(is.na(Bsmt.Qual), 'No Basement', as.character(Bsmt.Qual)),
  Bsmt.Cond = if_else(is.na(Bsmt.Cond), 'No Basement', as.character(Bsmt.Cond)),
  Bsmt.Exposure = if_else(is.na(Bsmt.Exposure), 'No Basement', as.character(Bsmt.Cond)),
  BsmtFin.Type.1 = if_else(is.na(BsmtFin.Type.1), 'No Basement', as.character(BsmtFin.Type.1)),
  BsmtFin.Type.2 = if_else(is.na(BsmtFin.Type.2), 'No Basement', as.character(BsmtFin.Type.2)),
  Fireplace.Qu = if_else(is.na(Fireplace.Qu), 'No Fireplace', as.character(Fireplace.Qu)),
  Garage.Type = if_else(is.na(Garage.Type), 'No Garage', as.character(Garage.Type)),
  Garage.Finish = if_else(is.na(Garage.Finish), 'No Garage', as.character(Garage.Finish)),
  Garage.Qual = if_else(is.na(Garage.Qual), 'No Garage', as.character(Garage.Qual)),
  Garage.Cond = if_else(is.na(Garage.Cond), 'No Garage', as.character(Garage.Cond)),
  Pool.QC = if_else(is.na(Pool.QC), 'No Pool', as.character(Pool.QC)),
  Fence = if_else(is.na(Fence), 'No Fence', as.character(Fence)),
  Misc.Feature = if_else(is.na(Misc.Feature), 'No MiscFeature', as.character(Misc.Feature))
)
```


```{r initmodel_test}
predInitialModel.BIC.test <-exp(predict(initialModel.BIC,ames_test)) 
initialModel.BIC.resid.test <- ames_test$price - predInitialModel.BIC.test
intialModel.BIC.rmse.test <- sqrt(mean(initialModel.BIC.resid.test^2))
paste("The out-of-sample RMSE (Root Mean Squared Error) is $", format(round(intialModel.BIC.rmse.test)), "dollars")
```

The out-of-sample RMSE `22726` is higher than the in-sample RMSE `21890`. This explains that the model performs well in in-sample and performs with slightly higher RMSE (root mean squared error) in out-of-sample.

* * *

**Note to the learner:** If in real-life practice this out-of-sample analysis shows evidence that the training data fits your model a lot better than the test data, it is probably a good idea to go back and revise the model (usually by simplifying the model) to reduce this overfitting. For simplicity, we do not ask you to do this on the assignment, however.

## Part 3 Development of a Final Model

Now that you have developed an initial model to use as a baseline, create a final model with *at most* 20 variables to predict housing prices in Ames, IA, selecting from the full array of variables in the dataset and using any of the tools that we introduced in this specialization.  

Carefully document the process that you used to come up with your final model, so that you can answer the questions below.

```{r}
names(ames_train)
```


```{r}
finalModel <- lm(log(price) ~ Overall.Qual + log(Garage.Area + 1) +   
                  log(Total.Bsmt.SF + 1) + Garage.Cars + log(area) + 
                  Full.Bath + Half.Bath + 
                  Bedroom.AbvGr + Year.Built + log(X1st.Flr.SF) + 
                  log(X2nd.Flr.SF + 1) +
                  log(Lot.Area) +  Central.Air + Overall.Cond + Land.Slope + Year.Remod.Add
                  + Bsmt.Qual,
                 data = ames_train)
```


```{r}
finalModel.AIC <- stepAIC(finalModel, k = 2, trace = FALSE, direction = "backward")
finalModel.BIC <- stepAIC(finalModel, k = log(nrow(ames_train)), direction = "backward", trace = FALSE)
```



### Section 3.1 Final Model

Provide the summary table for your model.

* * *
Our final model explains 91.67% variability with adjusted r-squared value of 0.9167. Full model started with 17 predictors and ended with 12 predictors yielding an highest adjusted r-squared value of 0.9167

however, model selection BIC, ended with 11 predictors yielding an adjusted r-squared value of 0.9161

```{r model_playground}
summary(finalModel.AIC)
```

```{r}
summary(finalModel.BIC)
```

```{r}
anova(initialModel.AIC, finalModel.AIC)
```


* * *

### Section 3.2 Transformation

Did you decide to transform any variables?  Why or why not? Explain in a few sentences.

* * *

Yes. I've decided to transform Lot.Area, area, X1st.Flr.SF to their log versions log(Lot.Area), log(area), log(X1st.Flr.SF) as the natural log of these predictors provide better linearity to the result.

I've visualized the same in the below plots (one with log transformation and another one without log transformation)

```{r model_assess}
ames_train %>% ggplot(aes(x = area, y = price)) + geom_point() + labs(title = "Area Vs Price", x = "Area", y = "Price")
```

```{r}
ames_train %>% ggplot(aes(x = log(area), y = log(price))) + geom_point() + labs(title = "Log(Area) Vs Log(Price)", x = "Log(Area)", y = "Log(Price)")
```

From the above plots, log transformations of area vs price depicts better linearity.
* * *

### Section 3.3 Variable Interaction

Did you decide to include any variable interactions? Why or why not? Explain in a few sentences.

* * *

I've not included any variable interactions as introducing interaction by creating a new variable called as `age` from the predictor `Year.Built` would not have a significant difference.

```{r}
ames_train %>% ggplot(aes(x = Year.Built, y = price)) + geom_point() 
```

```{r}
ames_train %>% mutate(age = max(Year.Built) - Year.Built) %>% ggplot(aes(x = age, y = price)) + geom_point()
```


* * *

### Section 3.4 Variable Selection

What method did you use to select the variables you included? Why did you select the method you used? Explain in a few sentences.

* * *

I've used AIC and BIC variable selection with backward elimination from the full model provided with 17 variables. Comparitatively, AIC selecton yielded better results with 12 variables whereas with 11 variables BIC selection yeielded little lesser adjusted r-squared value. As AIC explains 91.67% of variability in the target variable , I've decided to use AIC method.

```{r}
anova(finalModel.AIC,finalModel.BIC)
```

```{r}
summary(finalModel.AIC)
```

```{r}
summary(finalModel.BIC)
```

The summary results above interprets that the AIC method is better starting with 17 variables and using backward elimination, ended with 12 variables.

* * *

### Section 3.5 Model Testing

How did testing the model on out-of-sample data affect whether or how you changed your model? Explain in a few sentences.

* * *

This question refers how the models BIC and AIC performs on out-of-sample data.
Let's calculate RMSE (Root Mean Squared Error) for both of the final models in out-of-sample data and compare the results.

```{r}
predFinalModel.BIC <-exp(predict(finalModel.BIC,ames_test)) 
finalModel.BIC.resid <- ames_test$price - predFinalModel.BIC
finalModel.BIC.rmse <- sqrt(mean(finalModel.BIC.resid^2))
paste("The out-of-sample RMSE (Root Mean Squared Error) is $", format(round(finalModel.BIC.rmse)), "dollars")
```

```{r model}
predFinalModel.AIC <-exp(predict(finalModel.AIC,ames_test)) 
finalModel.AIC.resid <- ames_test$price - predFinalModel.AIC
finalModel.AIC.rmse <- sqrt(mean(finalModel.AIC.resid^2))
paste("The out-of-sample RMSE (Root Mean Squared Error) is $", format(round(finalModel.AIC.rmse)), "dollars")
```

From the results above, it's clear that model built using BIC selection performs better on the test data whereas AIC MODEL performs little lesser. It's clear from the RMSE value of BIC and AIC. model selected using AIC have RMSE 21457 which is higher than the RMSE 21316 of model selected using BIC.  
* * *

## Part 4 Final Model Assessment

### Section 4.1 Final Model Residual

For your final model, create and briefly interpret an informative plot of the residuals.

```{r}
plot(finalModel.BIC)
```

```{r}
plot(finalModel.BIC.resid)
```

The residuals are plotted around zero. Let's plot a graph between actual values and fitted values.
 
```{r}
plot(log(ames_train$price) ~ finalModel.BIC$fitted.values)
```

```{r}
plot(finalModel.BIC$residuals ~ finalModel.BIC$fitted.values)
```


* * *

The Nearly normal condition is met.
The Constant variability of residuals is around zero.
We can see that the model performs good with the above model diagnostic plots.

* * *

### Section 4.2 Final Model RMSE

For your final model, calculate and briefly comment on the RMSE.

* * *

```{r}
predFinalModel.BIC <-exp(predict(finalModel.BIC,ames_test)) 
finalModel.BIC.resid <- ames_test$price - predFinalModel.BIC
finalModel.BIC.rmse <- sqrt(mean(finalModel.BIC.resid^2))
paste("The out-of-sample RMSE (Root Mean Squared Error) is $", format(round(finalModel.BIC.rmse)), "dollars")
```

The RMSE of the final model selected using BIC yeilds 21316

```{r}
predFinalModel.AIC <-exp(predict(finalModel.AIC,ames_test)) 
finalModel.AIC.resid <- ames_test$price - predFinalModel.AIC
finalModel.AIC.rmse <- sqrt(mean(finalModel.AIC.resid^2))
paste("The out-of-sample RMSE (Root Mean Squared Error) is $", format(round(finalModel.AIC.rmse)), "dollars")
```

The RMSE of the final model selected using AIC yields 21457

* * *

### Section 4.3 Final Model Evaluation

What are some strengths and weaknesses of your model?

* * *

Strengths:
* The model selected using BIC have 1 predictor lesser than the model selected using AIC. The model which have lesser predictors performs better. But the model selected using BIC explains less variability in the response variable. 
* The model have adjusted r-squared value of 0.9167 which says that the model explains 91.67% percent variability in the response variable


Weaknesses:
* The model would have missed other predictors which explains better variability
* The RMSE can still be minimized by which the model can perform better on out-of-sample data
* The outliers should have been taken care of.

* * *

### Section 4.4 Final Model Validation

Testing your final model on a separate, validation data set is a great way to determine how your model will perform in real-life practice. 

You will use the “ames_validation” dataset to do some additional assessment of your final model. Discuss your findings, be sure to mention:
* What is the RMSE of your final model when applied to the validation data?  
* How does this value compare to that of the training data and/or testing data?
* What percentage of the 95% predictive confidence (or credible) intervals contain the true price of the house in the validation data set?  
* From this result, does your final model properly reflect uncertainty?

```{r loadvalidation, message = FALSE}
load("ames_validation.Rdata")
```

Let's first clean the validation dataset.

```{r}
ames_validation <- ames_validation %>% mutate(MS.SubClass = as.factor(MS.SubClass), Overall.Qual = as.factor(Overall.Qual), Overall.Cond = as.factor(Overall.Cond))
```

```{r}
ames_validation <- ames_validation %>% mutate(
  Alley = if_else(is.na(Alley), 'No Alley', as.character(Alley)),
  Bsmt.Qual = if_else(is.na(Bsmt.Qual), 'No Basement', as.character(Bsmt.Qual)),
  Bsmt.Cond = if_else(is.na(Bsmt.Cond), 'No Basement', as.character(Bsmt.Cond)),
  Bsmt.Exposure = if_else(is.na(Bsmt.Exposure), 'No Basement', as.character(Bsmt.Cond)),
  BsmtFin.Type.1 = if_else(is.na(BsmtFin.Type.1), 'No Basement', as.character(BsmtFin.Type.1)),
  BsmtFin.Type.2 = if_else(is.na(BsmtFin.Type.2), 'No Basement', as.character(BsmtFin.Type.2)),
  Fireplace.Qu = if_else(is.na(Fireplace.Qu), 'No Fireplace', as.character(Fireplace.Qu)),
  Garage.Type = if_else(is.na(Garage.Type), 'No Garage', as.character(Garage.Type)),
  Garage.Finish = if_else(is.na(Garage.Finish), 'No Garage', as.character(Garage.Finish)),
  Garage.Qual = if_else(is.na(Garage.Qual), 'No Garage', as.character(Garage.Qual)),
  Garage.Cond = if_else(is.na(Garage.Cond), 'No Garage', as.character(Garage.Cond)),
  Pool.QC = if_else(is.na(Pool.QC), 'No Pool', as.character(Pool.QC)),
  Fence = if_else(is.na(Fence), 'No Fence', as.character(Fence)),
  Misc.Feature = if_else(is.na(Misc.Feature), 'No MiscFeature', as.character(Misc.Feature))
)
```


```{r}
predFinalModel.BIC.validation <-exp(predict(finalModel.BIC,ames_validation)) 
finalModel.BIC.resid.validation <- ames_validation$price - predFinalModel.BIC.validation
finalModel.BIC.rmse.validation <- sqrt(mean(finalModel.BIC.resid.validation^2))
paste("The out-of-sample RMSE (Root Mean Squared Error) on validation dataset is $", format(round(finalModel.BIC.rmse.validation)), "dollars")
```

```{r}
predFinalModel.BIC.test <-exp(predict(finalModel.BIC,ames_test)) 
finalModel.BIC.resid.test <- ames_test$price - predFinalModel.BIC.test
finalModel.BIC.rmse.test <- sqrt(mean(finalModel.BIC.resid.test^2))
paste("The out-of-sample RMSE (Root Mean Squared Error) on test dataset is $", format(round(finalModel.BIC.rmse.test)), "dollars")
```

```{r}
predFinalModel.BIC.train <-exp(predict(finalModel.BIC,ames_train)) 
finalModel.BIC.resid.train <- ames_train$price - predFinalModel.BIC.train
finalModel.BIC.rmse.train <- sqrt(mean(finalModel.BIC.resid.train^2))
paste("The out-of-sample RMSE (Root Mean Squared Error) on training dataset is $", format(round(finalModel.BIC.rmse.train)), "dollars")
```

RMSE compared results with training dataset, test dataset and validation dataset:

"The out-of-sample RMSE (Root Mean Squared Error) on validation dataset is $ 20606 dollars"

"The out-of-sample RMSE (Root Mean Squared Error) on test dataset is $ 21316 dollars"

"The out-of-sample RMSE (Root Mean Squared Error) on training dataset is $ 20074 dollars"

* * *

* What is the RMSE of your final model when applied to the validation data?  

Ans: $20606 dollars

* How does this value compare to that of the training data and/or testing data?

Ans: RMSE of validation dataset ($20606) is greater than the RMSE of training dataset ($20074 dollars) as expected but lesser than the RMSE of the test dataset ($21316 dollars).

* What percentage of the 95% predictive confidence (or credible) intervals contain the true price of the house in the validation data set?

```{r}
# Predict prices
predict.final <- exp(predict(finalModel.BIC, ames_validation, interval = "prediction"))

# Calculate proportion of observations that fall within prediction intervals
coverage.prob.final <- mean(ames_validation$price > predict.final[,"lwr"] &
                            ames_validation$price < predict.final[,"upr"])
coverage.prob.final
```


* From this result, does your final model properly reflect uncertainty?

Yes indeed. only 0.0484928 proportion have price that falls outside the prediction intervals. 

* * *

## Part 5 Conclusion

Provide a brief summary of your results, and a brief discussion of what you have learned about the data and your model. 

* * *



* * *
